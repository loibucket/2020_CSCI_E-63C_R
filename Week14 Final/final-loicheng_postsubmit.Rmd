---
title: 'CSCI E-63C: Final Exam/Project'
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(GGally)
library(tidyverse)
library(corrplot)
library(psych)
library(PerformanceAnalytics)
library(MASS)
library(randomForest)
library(e1071)
library(class)
library(dummies)
library(neuralnet)
library(dplyr)
```

# Preface

For the final exam/project we will develop classification models using several approaches and compare their performance on a new dataset that is a subset of one of the datasets used in machine learning common task framework (CTF) competitions.  A copy of it split into training (`final-data-train.csv`, with outcome `response` available) and test (`final-data-test.csv`, stripped of the outcome, for prediction purposes only) datasets is available on our course website in Canvas as a zip-archive of all associated files.

Please notice, that at the end of this final exam/project you will be asked, in addition to the Rmarkdown and HTML files, to also make predictions for the observations in the *test* (not training!) dataset and upload them into Canvas as well.  The expected format for the file with predictions for test dataset is two columns of comma-separated values, one row per observation in *test* dataset, first column -- the observation identifier (column `id` in test dataset) and the second column -- your best model predictions for each observation in the *test* dataset as Y/N indicator.  To illustrate expected format the zip archive contains also a couple of examples of test predictions in this format for your reference as well (`predictions-*.csv` files in `predictions-examples` sub-folder in zip-archive).

One more time, to iterate and emphasize, please notice that this time your submission must consist of the following *three* (not just two, Rmd+html, as usual) items:

* Rmarkdown *.Rmd file with all the calculations you want to receive credit for,
* HTML version of the output generated by your *.Rmd file, and
* **predictions** for the **test** dataset in comma-separated values (CSV) format (file name **must** have *.csv extension for the file to load in Canvas)

The teaching team invites you to load your predictions (just predictions, just for the test dataset according to the file format shown in the sample files in the zip-archive) into Canvas repeatedly as you work on your models and improve them over the course of this week.  At least daily (or more frequently as we see fit) we will download predictions loaded by everyone by that time and compile a leaderboard in html format of all of them sorted by their accuracy as compared to the true values of the outcome for the test dataset (along with their sensitivity, specificity, etc.).  This list will be made available on our course website in Canvas for everyone in this class in order to see how the performance of their models compares across the rest of the models built by other students in the class.  The first version of the leaderboard posted on the course website at the time when final exam is made available starts with predictions made by those few example files provided in the zip-archive (coin flip, majority vote, etc. -- what do you think Charlie Brown is using to make the predictions?).  Those should be pretty easy to improve upon.

It is 100% up to you whether you want to upload your model predictions over the course of this week, how frequently you want to do it and what you want its results to be called in the leaderboard posted for everyone in the class to see.  We will use the name of the 2nd column in the file with predictions (the one containing Y/N values, not the numerical ids of the observations) as the model name listed in the leaderboard.  If you prefer not to use your name, choose something else instead, sufficiently unique so that it is easier for you to spot your result among all others.  Once again, please check out sample files of dull (majority vote, coin flip, etc.) predictions we have made available and consider how they show up in the leaderboard html file already posted on Canvas website.  Once you are done with final you are expected to load predictions from your best model into Canvas -- that is part of your points total as explained below.

Lastly, the size of this dataset can make some of the modeling techniques run slower than what we were typically encountering in this class.  You may find it helpful to do some of the exploration and model tuning on multiple random samples of smaller size as you decide on useful ranges of parameters/modeling choices, and then only perform a final run of fully debugged and working code on the full dataset.  Please see also the afterword below on the computational demands of this problem set.

# Problem 1: univariate and unsupervised analysis (20 points)

Download and read training and test data into R and prepare graphical and numerical summaries of it: e.g. histograms of continuous attributes, contingency tables of categorical variables, scatterplots of continuous attributes with some of the categorical variables indicated by color/symbol shape, etc.  Whatever you find helpful to think about properties of the data you are about to start using for fitting classification models.

As it is often the case for such contests, the attributes in the dataset are blinded in the sense that no information is available about what those are or what their values mean.  The only information available is that the attribute `response` is the outcome to be modeled and the attribute `id` is the unique numerical identifier for each observation.  Some of the remaining attributes are clearly categorical (those that are character valued) and some rather obviously continuous (those with numerical values with large number of unique values).  For several of them it is less clear whether it is best to treat them as continuous or categorical -- e.g. their values are numerical but there are relatively few unique values with many observations taking the same value, so that they arguably could be treated as continuous or categorical.  Please idenify them, reflect on how you prefer to handle them and describe this in your own words.

Perform principal components analysis of this data (do you need to scale it prior to that? how would you represent multilevel categorical attributes to be used as inputs for PCA?) and plot observations in the space of the first few principal components indicating levels of some of the categorical attributes of your choosing by the color/shape of the symbol.  Perform univariate assessment of associations between the outcome we will be modeling and each of the attributes (e.g. t-test or logistic regression for continuous attributes, contingency tables/Fisher exact test/$\chi^2$ test for categorical attributes).  Summarize your observations from these assessments: does it appear that there are predictors associated with the outcome `response` univariately? Which predictors seem to be more/less relevant?

```{r first}
df = read.csv("final-exam-data/final-data-train.csv")

##binary factors
df$response = as.factor(df$response)
df$ent = as.factor(df$ent)
df$wi = as.factor(df$wi)
df$np = as.factor(df$np)

##more than 2 factors
df$wc = as.factor(df$wc)
df$zwp = as.factor(df$zwp)
df$bnf = as.factor(df$bnf)
df$tdt = as.factor(df$tdt)
df$sb = as.factor(df$sb)
df$ox = as.factor(df$ox)
df$xt = as.factor(df$xt)
df$ku = as.factor(df$ku)

#df
```

```{r}
dt = read.csv("final-exam-data/final-data-test.csv")
#dt
```

```{r}
dtmp=NULL
for (col in colnames(df)){
  dtmp = cbind(dtmp, c(var=col,uniques=nrow(unique(df[col]))))
}
data.frame(dtmp)
```
Checking the number of unqiue values, id, qh and is looks like continuous, by the high number of distinct values.  All the other variables have very low distinct values, and so could be categorical.


```{r include=FALSE, eval=FALSE}
#df %>% select(-contains(c("wc","zwp","wi","bnf","ent")))
```

The pairs plots and histograms are created below.  All factor variables with more than 2 cases are converted to integers, to check if they should be continuous instead.

```{r fig.width=30,fig.height=30}

##sample limited data for faster plots
set.seed(1)
dsample = df[sample(nrow(df),1000),]

##more than 2 factors, try as integers
dsample$wc = as.integer(dsample$wc)
dsample$zwp = as.integer(dsample$zwp)
dsample$bnf = as.integer(dsample$bnf)
dsample$tdt = as.integer(dsample$tdt)
dsample$sb = as.integer(dsample$sb)
dsample$ox = as.integer(dsample$ox)
dsample$xt = as.integer(dsample$xt)
dsample$ku = as.integer(dsample$ku)

# Customize cor panel
cor.panel<-function(x, y){
  points(x,y, pch = 19, col = c("blue","red")[dsample$response])
}
# Create the plots
pairs(dsample, 
      lower.panel = cor.panel,
      upper.panel = cor.panel)
```


```{r fig.width=30,fig.height=30}
##sample limited data for faster plots
set.seed(1)
dsample = df[sample(nrow(df),1000),]

pairs.panels(dsample, 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE, # show correlation ellipses
             bg=c("blue","red")[dsample$response],
             scale = TRUE
             )
```

From reviewing the pairs plots, the following is observed:

      [,1]      
 [1,] "response" -> categorical
 [2,] "id" -> continuous, shown from pairs plot with "is"
 [3,] "wc" -> categorical, distinct lines in all plots
 [4,] "zwp" -> categorical     
 [5,] "wi" -> categorical      
 [6,] "dtj" -> continuous, shows distinct lines, but looks correlated with "qh" 
 [7,] "bnf" -> categorical     
 [8,] "qh" -> continuous, with "is" "id"      
 [9,] "ent" -> categorical
[10,] "sci" -> continuous, with "is" "id" 
[11,] "ypz" -> categorical, shows distinct lines, histogram distribution is unusual       
[12,] "bw" -> categorical, shows distinct lines        
[13,] "tdt" -> categorical, shows distinct lines        
[14,] "sb" -> categorical, shows distinct lines        
[15,] "ox" -> categorical, shows distinct lines        
[16,] "xt" -> categorical, shows distinct lines         
[17,] "np" -> categorical, shows distinct lines         
[18,] "ku" -> categorical, shows distinct lines         
[19,] "is" -> continuous, shown from pairs plot with "id"

So, the only number type variable that is converted to a categorical is "ypz"

```{r}
df$ypz = as.factor(df$ypz)
```

The PCA plots are below
```{r pca, fig.width=12,fig.height=4,warning=FALSE}
ps = prcomp(dummy.data.frame(df[2:length(df)]),retx=TRUE,scale=TRUE)
op = par(mfrow=c(1,3))
plot(ps$x[,1:2],col=df$response,ylim=c(-8,4),xlim=c(-5,15))
plot(ps$x[as.character(df$response) == "Y",1:2],ylim=c(-8,4),xlim=c(-5,15),col=2)
plot(ps$x[as.character(df$response) == "N",1:2],ylim=c(-8,4),xlim=c(-5,15),col=1)
par(op)
```
From the plots above, it does not seem that the Y and N responses can be separated by PC1 and PC2 alone.  Both sets of data appears to overlap.


```{r fig.width=16,fig.height=8}
op=par(mfrow=c(1,2))
"PC1"
n = names(sort(abs(ps$rotation[,1]),decreasing=TRUE)[c(1:20)])
tops = ps$rotation[,1][n]
tops
barplot(abs(tops),las=2, main="PC1 weights (abs)")

"PC2"
n = names(sort(abs(ps$rotation[,2]),decreasing=TRUE)[c(1:20)])
tops = ps$rotation[,2][n]
tops
barplot(abs(tops),las=2,main="PC2 weights (abs)")
par(op)
```

From PCA, we find that "qh", "bw" are large contributors to PC1 and "wcb" "wcd" to PC2.

# Problem 2: logistic regression (20 points)

Develop logistic regression model of the outcome `response` as a function of multiple predictors in the model.  Which variables are significantly associated with the outcome?  Test model performance on multiple splits of data into training and test subsets and summarize it in terms of accuracy/error/sensitivity/specificity.

```{r fig.width=8,fig.height=8,warning=FALSE,message=FALSE}
#Logistic regression, all data
op = par(mfrow=c(2,2))
fit = glm(response~.,data=df,family=binomial())
#summary(fit)
plot(fit)

cook = sort(cooks.distance(fit),decreasing=TRUE) 
head(data.frame( cook ), 10)
plot( cook )
plot( cook[2:length(cook)] )
plot( cook[4:length(cook)] )
plot( cook[6:length(cook)] )
par(op)
```

Point 7195 has unusually high leverage.  We can try to remove the high leverage points from the training data.

```{r fig.width=8,fig.height=12,warning=FALSE,message=FALSE}
op = par(mfrow=c(3,2))

exclude = c(as.integer(names(cook[1:1])),10308)
exclude

#Logistic regression, split data
cvdex = sort(  unique( c(sample(nrow(df),floor(0.2*nrow(df))),exclude) )  )  ##exclude leverage points from training
dtrain = df[-cvdex,] 
dcv = df[cvdex,]

fit = glm(response~.,data=dtrain,family=binomial())
summary(fit)
plot(fit)

p = predict(fit, newdata=dcv, type="response")
dtmp = data.frame(dcv$response,as.factor(ifelse(p>0.5,"Y","N")))

tab = table(dtmp)
accu = sum(diag(tab))/sum(tab)
TPRsens = tab[2,2]/sum(tab[2,])
TNRspec = tab[1,1]/sum(tab[1,])
prec = tab[2,2]/sum(tab[,2])

tab
accu
TPRsens
TNRspec
prec

plot( head(  sort(cooks.distance(fit),decreasing=TRUE)  , 100  ) )
par(op)
```

The residual plot looks more reasonable with the high leverage points removed

```{r logres, fig.width=10,fig.height=10,warning=FALSE}
#Log Metrics Histograms
op = par(mfrow=c(2,2))
accu = NULL
TPRsens = NULL
TNRspec = NULL
prec = NULL

for (count in 1:6){
  cvdex = sort(  unique( c(sample(nrow(df),floor(0.2*nrow(df))),exclude) )  )  ##exclude leverage points from training
  dtrain = df[-cvdex,] 
  dcv = df[cvdex,]
  
  fit = glm(response~.,data=dtrain,family=binomial())
  p = predict(fit, newdata=dcv, type="response")
  dtmp = data.frame(dcv$response,as.factor(ifelse(p>0.5,"Y","N")))

  tab = table(dtmp)
  accu = c(accu,sum(diag(tab))/sum(tab))
  TPRsens = c(TPRsens,tab[2,2]/sum(tab[2,]))
  TNRspec = c(TNRspec,tab[1,1]/sum(tab[1,]))
  prec = c(prec,tab[2,2]/sum(tab[,2]))
}

log_accu = accu
log_TPRsens = TPRsens
log_TNRspec = TNRspec
log_prec = prec
```

```{r fig.width=10,fig.height=10,warning=FALSE}
#Log reg
op = par(mfrow=c(2,2))
hist(accu)
hist(TPRsens)
hist(TNRspec)
hist(prec)
par(op)
```

The logistic regression model as is looks good for prediction.  All the metrics have high values, about over 80%.  However, sensitivity is not very good, at around 60%.


## Extra points problem: interaction terms (5 extra points)

Assess the impact/significance of pairwise interaction terms for all pairwise combinations of covariates used in the model and report the top ten that most significantly improve model fit.

# Problem 3: linear discriminant analysis (15 points)

Fit linear discriminant analysis model of the outcome `response` as a function of the rest of covariates in the dataset.  Feel free to decide whether you want to use all of them or a subset of those.  Test resulting model performance on multiple splits of the data into training and test subsets, summarize it in terms of accuracy/error/sensitivity/specificity and compare them to those obtained for logistic regression.

```{r fig.width=4,fig.height=8}
##LDA
cvdex = sort(  unique( c(sample(nrow(df),floor(0.2*nrow(df))),exclude) )  )  ##exclude leverage points from training
dtrain = df[-cvdex,] 
dcv = df[cvdex,]

fit = lda(response~.,data=dtrain)
summary(fit)
plot(fit)

p = predict(fit, newdata=dcv, type="response")
p = data.frame(p)[,1] ##lda only
dtmp = data.frame(dcv$response,p)

tab = table(dtmp)
accu = sum(diag(tab))/sum(tab)
TPRsens = tab[2,2]/sum(tab[2,])
TNRspec = tab[1,1]/sum(tab[1,])
prec = tab[2,2]/sum(tab[,2])

tab
accu
TPRsens
TNRspec
prec
```

```{r lda, fig.width=10,fig.height=10,warning=FALSE}
#LDA Metrics Histograms
op = par(mfrow=c(2,2))
accu = NULL
TPRsens = NULL
TNRspec = NULL

for (count in 1:6){
  cvdex = sort(  unique( c(sample(nrow(df),floor(0.2*nrow(df))),exclude) )  )  ##exclude leverage points from training
  dtrain = df[-cvdex,] 
  dcv = df[cvdex,]
  
  fit = lda(response~.,data=dtrain)
  p = predict(fit, newdata=dcv, type="response")
  p = data.frame(p)[,1]  ##lda only
  dtmp = data.frame(dcv$response,p)

  tab = table(dtmp)
  accu = c(accu,sum(diag(tab))/sum(tab))
  TPRsens = c(TPRsens,tab[2,2]/sum(tab[2,]))
  TNRspec = c(TNRspec,tab[1,1]/sum(tab[1,]))
  prec = c(prec,tab[2,2]/sum(tab[,2]))
}

lda_accu = accu
lda_TPRsens = TPRsens
lda_TNRspec = TNRspec
lda_prec = prec
```

```{r fig.width=10,fig.height=10,warning=FALSE}
##LDA
op = par(mfrow=c(2,2))
hist(accu)
hist(TPRsens)
hist(TNRspec)
hist(prec)
par(op)
```

The logistic regression model as is looks good for prediction.  All the metrics have high values, about over 80%.  It is comparable to the logistic regression model.  Similarly, it also has lower sensitivity, around 60%.

# Problem 4: random forest (15 points)

Develop random forest model of outcome `response`. Present variable importance plots and comment on relative importance of different attributes in the model.  Did attributes showing up as more important in random forest model also appear as significantly associated with the outcome by logistic regression?  Test model performance on multiple splits of data into training and test subsets, compare test and out-of-bag error estimates, summarize model performance in terms of accuracy/error/sensitivity/specificity and compare to the performance of logistic regression and LDA models above.

```{r warning=FALSE}
cvdex = sort(  unique( c(sample(nrow(df),floor(0.2*nrow(df))),exclude) )  )  ##exclude leverage points from training
dtrain = df[-cvdex,] 
dcv = df[cvdex,]

ytrain = dtrain[,1]
dtrain = dtrain[,2:length(dtrain)]
ycv = dcv[,1]
dcv = dcv[,2:length(dcv)]
fit = randomForest(dtrain,ytrain)
p = predict(fit,newdata=dcv)
```

```{r}
dtmp = data.frame(ycv,p)

tab = table(dtmp)
accu = sum(diag(tab))/sum(tab)
TPRsens = tab[2,2]/sum(tab[2,])
TNRspec = tab[1,1]/sum(tab[1,])
prec = tab[2,2]/sum(tab[,2])

tab
accu
TPRsens
TNRspec
prec
```

```{r rf, fig.width=10,fig.height=10,warning=FALSE}
#RF Metrics Histograms
op = par(mfrow=c(2,2))
accu = NULL
TPRsens = NULL
TNRspec = NULL

for (count in 1:4){
  cvdex = sort(  c(sample(nrow(df),floor(0.2*nrow(df))),exclude)  )  ##exclude leverage points from training
  dtrain = df[-cvdex,] 
  dcv = df[cvdex,]
  
  ##DEBUG sample limited data for faster plots
  #dtrain = df[sample((1:nrow(df))[-cvdex],10000),] 
  #dcv = df[sample((1:nrow(df))[cvdex],100),]   
  
  ytrain = dtrain[,1]
  dtrain = dtrain[,2:length(dtrain)]
  ycv = dcv[,1]
  dcv = dcv[,2:length(dcv)]
  fit = randomForest(dtrain,ytrain)
  p = predict(fit,newdata=dcv)
  dtmp = data.frame(ycv,p)
  
  tab = table(dtmp)
  accu = c(accu,sum(diag(tab))/sum(tab))
  TPRsens = c(TPRsens,tab[2,2]/sum(tab[2,]))
  TNRspec = c(TNRspec,tab[1,1]/sum(tab[1,]))
  prec = c(prec,tab[2,2]/sum(tab[,2]))
}

rf_accu = accu
rf_TPRsens = TPRsens
rf_TNRspec = TNRspec
rf_prec = prec
```

```{r fig.width=10,fig.height=10,warning=FALSE}
#Random Forest
op = par(mfrow=c(2,2))
hist(accu)
hist(TPRsens)
hist(TNRspec)
hist(prec)
par(op)
```

The rf model is very good.  All metrics appear reasonable, above 80pct.  The sensitivity is higher than LDA and logistic.

# Problem 5: SVM (20 points)

Develop SVM model of categorical outcome `response` deciding on the choice of kernel, cost, etc. that appear to yield better performance.  Test model performance on multiple splits of data into training and test subsets, summarize model performance in terms of accuracy/error/sensitivity/specificity and compare to the performance of the rest of the models developed above (logistic regression, LDA, random forest).

Due to the very long runtime, the model was created from random sample of data points, instead of using the entire dataset.

```{r warning=FALSE}
cvdex = sort(  unique( c(sample(nrow(df),floor(0.2*nrow(df))),exclude) )  )  ##exclude leverage points from training
dtrain = df[-cvdex,] 
dcv = df[cvdex,]

##DEBUG sample limited data for faster plots
dtrain = df[sample((1:nrow(df))[-cvdex],10000),] 
#dcv = df[sample((1:nrow(df))[cvdex],100),]    

fit = svm(response~.,data=dtrain,kernel="linear", cost=0.001)
summary(fit)

p = predict(fit,newdata=dcv)
dtmp = data.frame(dcv$response,p)

tab = table(dtmp)
accu = sum(diag(tab))/sum(tab)
TPRsens = tab[2,2]/sum(tab[2,])
TNRspec = tab[1,1]/sum(tab[1,])
prec = tab[2,2]/sum(tab[,2])

tab
accu
TPRsens
TNRspec
prec
```

```{r svm, fig.width=10,fig.height=10,warning=FALSE}
#SVM Metrics Histograms
op = par(mfrow=c(2,2))
accu = NULL
TPRsens = NULL
TNRspec = NULL

for (count in 1:4){
  cvdex = sort(  unique( c(sample(nrow(df),floor(0.2*nrow(df))),exclude) )  )  ##exclude leverage points from training
  dtrain = df[-cvdex,] 
  dcv = df[cvdex,]
  
  ##DEBUG sample limited data for faster plots
  dtrain = df[sample((1:nrow(df))[-cvdex],10000),] 
  #dcv = df[sample((1:nrow(df))[cvdex],100),]      
  
  fit = svm(response~.,data=dtrain,kernel="linear",cost=0.01)
  summary(fit)
  
  p = predict(fit,newdata=dcv)
  dtmp = data.frame(dcv$response,p)
  
  tab = table(dtmp)
  accu = c(accu,sum(diag(tab))/sum(tab))
  TPRsens = c(TPRsens,tab[2,2]/sum(tab[2,]))
  TNRspec = c(TNRspec,tab[1,1]/sum(tab[1,]))
  prec = c(prec,tab[2,2]/sum(tab[,2]))
}

svm_accu = accu
svm_TPRsens = TPRsens
svm_TNRspec = TNRspec
svm_prec = prec
```

```{r fig.width=10,fig.height=10,warning=FALSE}
#SVM
op = par(mfrow=c(2,2))
hist(accu)
hist(TPRsens)
hist(TNRspec)
hist(prec)
par(op)
```

SVM performs well, similar to logistic and LDA.  It has lower sensitivity around 60%, just like logistic and LDA.

#knn
Due to the very long runtime, the model was created from random sample of data points, instead of using the entire dataset.

```{r warning=FALSE}
##knn
cvdex = sort(  unique( c(sample(nrow(df),floor(0.2*nrow(df))),exclude) )  )  ##exclude leverage points from training
dtrain = df[-cvdex,] 
dcv = df[cvdex,]

##DEBUG sample limited data for faster plots
dtrain = df[sample((1:nrow(df))[-cvdex],8000),] 
#dcv = df[sample((1:nrow(df))[cvdex],100),]   

ytrain = dtrain[,1]
dtrain = dtrain[,2:length(dtrain)]
ycv = dcv[,1]
dcv = dcv[,2:length(dcv)]

tr = as.matrix(dummy.data.frame(dtrain,drop=FALSE))
cv = as.matrix(dummy.data.frame(dcv,drop=FALSE))
cla = ytrain

p = knn(tr,cv,cla,k=5)
summary(p)
dtmp = data.frame(ycv,p)
tab = table(dtmp)
accu = sum(diag(tab))/sum(tab)
TPRsens = tab[2,2]/sum(tab[2,])
TNRspec = tab[1,1]/sum(tab[1,])
prec = tab[2,2]/sum(tab[,2])

tab
accu
TPRsens
TNRspec
prec
```


```{r knn, fig.width=10,fig.height=10,warning=FALSE}
#knn Metrics Histograms
op = par(mfrow=c(2,2))
accu = NULL
TPRsens = NULL
TNRspec = NULL

for (count in 1:5){
  cvdex = sort(  unique( c(sample(nrow(df),floor(0.2*nrow(df))),exclude) )  )  ##exclude leverage points from training
  dtrain = df[-cvdex,] 
  dcv = df[cvdex,]
    
  ##DEBUG sample limited data for faster plots
  dtrain = df[sample((1:nrow(df))[-cvdex],8000),] 
  #dcv = df[sample((1:nrow(df))[cvdex],100),]    
  
  ytrain = dtrain[,1]
  dtrain = dtrain[,2:length(dtrain)]
  ycv = dcv[,1]
  dcv = dcv[,2:length(dcv)]
  
  tr = as.matrix(dummy.data.frame(dtrain,drop=FALSE))
  cv = as.matrix(dummy.data.frame(dcv,drop=FALSE))
  cla = ytrain
  
  p = knn(tr,cv,cla,k=5)
  summary(p)
  dtmp = data.frame(ycv,p)
  
  tab = table(dtmp)
  accu = c(accu,sum(diag(tab))/sum(tab))
  TPRsens = c(TPRsens,tab[2,2]/sum(tab[2,]))
  TNRspec = c(TNRspec,tab[1,1]/sum(tab[1,]))
  prec = c(prec,tab[2,2]/sum(tab[,2]))
}

knn_accu = accu
knn_TPRsens = TPRsens
knn_TNRspec = TNRspec
knn_prec = prec
```

```{r fig.width=10,fig.height=10,warning=FALSE}
#knn
op = par(mfrow=c(2,2))
hist(accu)
hist(TPRsens)
hist(TNRspec)
hist(prec)
par(op)
```

knn does not appear to perform well compared to the others.  It may need more tuning

#neural net

Due to the very long runtime, the nn model was created from random sample of data points, instead of using the entire dataset.

```{r warning=FALSE}
##neural net
cvdex = sort(  unique( c(sample(nrow(df),floor(0.2*nrow(df))),exclude) )  )  ##exclude leverage points from training
dtrain = df[-cvdex,] 
dcv = df[cvdex,]

##DEBUG sample limited data for faster plots
dtrain = df[sample((1:nrow(df))[-cvdex],9000),] 
#dcv = df[sample((1:nrow(df))[cvdex],100),]  

dtrain = dtrain %>% mutate_if(is.numeric, scale)
dcv = dcv %>% mutate_if(is.numeric, scale)

dtrain = data.frame(response=dtrain$response,dummy.data.frame(dtrain[2:length(dtrain)],drop=FALSE))
dcv = data.frame(response=dcv$response,dummy.data.frame(dcv[2:length(dcv)],drop=FALSE))

dtrain$response = as.integer(  as.integer(dtrain$response) - 1  )
dcv$response = as.integer(  as.integer(dcv$response) - 1  )

fit = neuralnet( response~. , dtrain, hidden=2, linear.output=FALSE, threshold = 0.05 )#, stepmax = 2e+04 )
summary(fit)
```

```{r}
plot(fit)

p = predict( fit, newdata=dcv )

dtmp = data.frame(dcv$response,as.factor(ifelse(p>0.5,"Y","N")))
tab = table(dtmp)
tab

accu = sum(diag(tab))/sum(tab)
TPRsens = tab[2,2]/sum(tab[2,])
TNRspec = tab[1,1]/sum(tab[1,])
prec = tab[2,2]/sum(tab[,2])

tab
accu
TPRsens
TNRspec
prec
```

```{r nn, fig.width=10,fig.height=10,warning=FALSE}
#neural net Metrics Histograms
op = par(mfrow=c(2,2))
accu = NULL
TPRsens = NULL
TNRspec = NULL

for (count in 1:3){
  ##neural net
  cvdex = sort(  unique( c(sample(nrow(df),floor(0.2*nrow(df))),exclude) )  )  ##exclude leverage points from training
  dtrain = df[-cvdex,] 
  dcv = df[cvdex,]
  
  ##DEBUG sample limited data for faster plots
  dtrain = df[sample((1:nrow(df))[-cvdex],9000),] 
  #dcv = df[sample((1:nrow(df))[cvdex],100),]  
    
  dtrain = dtrain %>% mutate_if(is.numeric, scale)
  dcv = dcv %>% mutate_if(is.numeric, scale)
  
  dtrain = data.frame(response=dtrain$response,dummy.data.frame(dtrain[2:length(dtrain)],drop=FALSE))
  dcv = data.frame(response=dcv$response,dummy.data.frame(dcv[2:length(dcv)],drop=FALSE))
  
  dtrain$response = as.integer(  as.integer(dtrain$response) - 1  )
  dcv$response = as.integer(  as.integer(dcv$response) - 1  )
  
  fit = neuralnet( response~. , dtrain, hidden=2, linear.output=FALSE, threshold = 0.05 )#, stepmax = 2e+04 )
  summary(fit)
  plot(fit)
  
  p = predict( fit, newdata=dcv )
  dtmp = data.frame(dcv$response,as.factor(ifelse(p>0.5,"Y","N")))
  
  tab = table(dtmp)
  accu = c(accu,sum(diag(tab))/sum(tab))
  TPRsens = c(TPRsens,tab[2,2]/sum(tab[2,]))
  TNRspec = c(TNRspec,tab[1,1]/sum(tab[1,]))
  prec = c(prec,tab[2,2]/sum(tab[,2]))
}

nn_accu = accu
nn_TPRsens = TPRsens
nn_TNRspec = TNRspec
nn_prec = prec
```

```{r fig.width=10,fig.height=10,warning=FALSE}
#neural net
op = par(mfrow=c(2,2))
accu
hist(accu)
hist(TPRsens)
hist(TNRspec)
hist(prec)
par(op)
```
EVen with using about half the data for the training, the nn model performs well, with all metrics about 70%.

# Problem 6: predictions for test dataset  (10 points)

## Problem 6a: compare logistic regression, LDA, random forest and SVM model performance (3 points)

Compare performance of the models developed above (logistic regression, LDA, random forest, SVM) in terms of their accuracy, error and sensitivity/specificity.  Comment on differences and similarities between them.

```{r boxplot, fig.width=13,fig.height=4,warning=FALSE}
op = par(mfrow=c(1,4))
colnam=c("log","lda","rf","svm","knn","nn")
boxplot(log_accu,lda_accu,lda_accu,svm_accu,knn_accu,nn_accu,   names=colnam,main="accuracy")
boxplot(log_TPRsens,lda_TPRsens,rf_TPRsens,svm_TPRsens,knn_TPRsens,nn_TPRsens,   names=colnam,main="TPR sensitivity")
boxplot(log_TNRspec,lda_TNRspec,rf_TNRspec,svm_TNRspec,knn_TNRspec,nn_TNRspec,   names=colnam,main="TNR specificty")
boxplot(log_prec,lda_prec,rf_prec,svm_prec,knn_prec,nn_prec,   names=colnam,main="precision")
par(op)
```

```{r fig.width=13,fig.height=4,warning=FALSE}
op = par(mfrow=c(1,4))
colnam=c("log","lda","rf","svm","nn")
boxplot(log_accu,lda_accu,lda_accu,svm_accu,nn_accu,   names=colnam,main="accuracy")
boxplot(log_TPRsens,lda_TPRsens,rf_TPRsens,svm_TPRsens,nn_TPRsens,   names=colnam,main="TPR sensitivity")
boxplot(log_TNRspec,lda_TNRspec,rf_TNRspec,svm_TNRspec,nn_TNRspec,   names=colnam,main="TNR specificty")
boxplot(log_prec,lda_prec,rf_prec,svm_prec,nn_prec,   names=colnam,main="precision")
par(op)
```

With all of them compared in the boxplots, random forest seems best.  All the other models have low performance in sensitivity.

## Problem 6b: make predictions for the **test** dataset (3 points)

Decide on the model that performs the best and use it to make predictions for the **test** dataset.  This is the dataset that is provided separately from training data without the outcome `response` that we are modeling here.  Upload resulting predictions in comma-separated values (CSV) format into the Canvas website.  Please check sample files with test dataset predictions for the expected format of the *.csv file with predictions: your submission must be in precisely the same format -- two and only two columns, first column - ids of the test observations ("id" column in test dataset), second - predictions as Y/N calls (not 0/1, 1/2, true/false, etc.).  The name of the second column of predictions is what will be used in leaderboard as its name.

```{r}
###reread data
df = read.csv("final-exam-data/final-data-train.csv")

##binary factors
df$response = as.factor(df$response)
df$ent = as.factor(df$ent)
df$wi = as.factor(df$wi)
df$np = as.factor(df$np)

##more than 2 factors
df$wc = as.factor(df$wc)
df$zwp = as.factor(df$zwp)
df$bnf = as.factor(df$bnf)
df$tdt = as.factor(df$tdt)
df$sb = as.factor(df$sb)
df$ox = as.factor(df$ox)
df$xt = as.factor(df$xt)
df$ku = as.factor(df$ku)

##numeric as factor
df$ypz = as.factor(df$ypz)

rbind(head(df,2),df[sample(nrow(df),1),]) ##get 1st rows and sample random rows
```

```{r last, fig.width=8, fig.height=4, warning=FALSE}
dtrain = df[-exclude,]
##DEBUG sample limited data for faster plots
#dtrain = df[sample((1:nrow(df))[-cvdex],1000),] 
#dcv = df[sample((1:nrow(df))[cvdex],100),]  
  
ytrain = dtrain[,1]
dtrain = dtrain[,2:length(dtrain)]
ycv = dcv[,1]
dcv = dcv[,2:length(dcv)]
fit = randomForest(dtrain,ytrain)

##run on test data
dt = read.csv("final-exam-data/final-data-test.csv")

##binary factors
#dt$response = as.factor(dt$response)
dt$ent = as.factor(dt$ent)
dt$wi = as.factor(dt$wi)
dt$np = as.factor(dt$np)

##more than 2 factors
dt$wc = as.factor(dt$wc)
dt$zwp = as.factor(dt$zwp)
dt$bnf = as.factor(dt$bnf)
dt$tdt = as.factor(dt$tdt)
dt$sb = as.factor(dt$sb)
dt$ox = as.factor(dt$ox)
dt$xt = as.factor(dt$xt)
dt$ku = as.factor(dt$ku)

##numeric as factor
dt$ypz = as.factor(dt$ypz)
rbind(head(dt,2),dt[sample(nrow(dt),1),]) ##get 1st rows and sample random rows

nrow(dt)

p = predict(fit,newdata=dt)
summary(p)

op=par(mfrow=c(1,2))
plot(p, main="predicted")
plot(df$response, main ="train data")
par(op)

#write out test predictions
#out = data.frame(p)
#pid = as.integer(rownames(out))
#pp = out$p
#writeout = data.frame(id=pid,loi=pp)
#write.csv(writeout,"predictions-lc-1.csv",row.names=FALSE)
```
```{r}
head(dt)
out=data.frame(p)
writeout = data.frame(id=dt$id,loi=p)
head(writeout)
write.csv(writeout,"predictions-lc-1_postsubmit.csv",row.names=FALSE)
```

## Problem 6c: get better than coin flip by 10% (4 points)

This is not really a problem *per se* but rather a criterion that we will go by when assessing quality of your predictions for the test dataset.  You get these four points if your predictions for **test** dataset are better than those obtained from a fair coin flip (already shown in leaderboard and as examples of the file format for predictions upload) by at least 10% on **all** four metrics shown in the leaderboard (accuracy, sensitivity, specificity and precision).  But then predictions by the coin flip should not be very difficult to improve upon.  

# Extra 5 points: KNN model

Develop KNN classifiers for various values of $K$ and compare their performance to that from the rest of the models evaluated herein.  The challenges associated with this task include: a) handling the increase in computational demands of KNN models with the increase in the size of the data, and b) deciding on the choice of the distance that is suitable for a dataset with categorical attributes. 

# Extra 10 points: neural network model

Experiment with fitting neural network models of categorical outcome `response` for this data and evaluate their performance on different splits of the data into training and test. Compare model performance to that for the rest of classifiers developed above.


# An afterword on the computational demands of the final exam

Because during previous offerings of this course there were always several posts on piazza regarding how long it takes to fit various classifiers to the final exam dataset we have added this note here.

First of all, we most definitely do *not* expect you to *have* to buy capacity from AWS to complete this assignment. You certainly can if you want to, but this course is not about that and this dataset is really not *that* big to require it. Something reasonable/useful can be accomplished for this data with middle of the road hardware. For instance, knitting of the entire official solution for the final exam on 8Gb RAM machine with two i5-7200u cores takes about an hour using single-threaded R/Rstudio and this includes both extra points problems as well as various assessments of the performance of different models as function of data size and so on.

Second, your solution should not take hours and hours to compile. If it does, it could be that it is attempting to do too much, or something is implemented inefficiently, or just plain incorrectly - it is impossible for us to comment on this until we see the code when we grade it. In general, it is often very prudent to "start small" -- fit your model on a random subset of data small enough for the model fitting call to return immediately, check how model performance (both in terms of error and time it takes to compute) scales with the size of the data you are training it on (as you increase it in size, say, two-fold several times), for tuning start with very coarse grid of parameter values and given those results decide what it right for you, etc.

Lastly, making the decision about what is right for the problem at hand, how much is enough, etc. is inherent in this line of work. If you choose to conduct model tuning on a subset of the data - especially if you have some assessment of how the choice of tuning parameter and test error is affected by the size of training dataset - it could be a very wise choice.  If it is more efficient for you to knit each problem separately, by all means feel free to do that - just remember to submit each .Rmd and HTML file that comprises your entire solution. On that note, if you end up using any of the unorthodox setups for your calculations (e.g. AWS, parallel processing, multiple machines, etc. - none of which are essential for solving it correctly) please be sure that when we grade we have every relevant piece of code available - we won't be able to grade your work if we are not clear about how the results were obtained.

In the end, the final exam asks you to assess performance of several classification technologies on a new dataset, decide on which classifier is the best and use it to make predictions for the test data. It is very much up to you how exactly you want to go about it.  There could be many versions of correct and informative solution for that (as there could be just as many if not more that are completely wrong).

As always, best of luck - we are practically done here!
