---
title: "CSCI-E63C: Week 12 Q&A section"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
ptStart <- proc.time()
library(ISLR)
library(e1071)
library(ROCR)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
```

# Ch.9.6.1 -- Support Vector Classifier

```{r Ch.9.6.1a}
set.seed (1)
nObs <- 10
x <- matrix (rnorm (2*nObs*2) , ncol =2)
y <- c(rep (-1,nObs) , rep (1 ,nObs) )
x[y==1 ,] <- x[y==1,] + 1
# not linearly separable:
plot(x, col =(3-y), pch =(3-y))
# outcome has to be a factor:
dat <- data.frame(x=x,y=as.factor(y))

# cost=10, no scaling this time:
svmfit <- svm(y~., data=dat, kernel="linear", cost=10, scale=FALSE)
# one(?) misclassification, "X" indicates SV:
plot(svmfit,dat)
# seven of them:
svmfit$index
summary(svmfit)
# actually three misclassifications:
table(predict(svmfit),y)
plot(x, col =(3-y),pch=as.numeric(predict(svmfit)))
text(-0.9,2.3-(0:2)/5,c("Truth:",as.character(unique(y))),col=c(1,unique(3-y)),pos=2)
text(2,-1.5,"Prediction:",pos=4)
legend("bottomright",levels(predict(svmfit)),pch=1:2,bty="n")

# lower cost=0.1, wider margin, more SVs:
svmfit <- svm(y~., data=dat, kernel="linear", cost=0.1, scale=FALSE)
plot(svmfit,dat)
# 16 of them:
svmfit$index
summary(svmfit)
table(predict(svmfit),y)
# now only one misclassification:
plot(x, col =(3-y),pch=as.numeric(predict(svmfit)))

# tune cost by cross-validation:
set.seed(1)
tune.out <- tune(svm, y~., data=dat, kernel="linear", ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
# cost=0.1 is the best:
summary(tune.out)
# best model:
bestmod <- tune.out$best.model
summary(bestmod)

# denser grid around minimum:
tune.out.1 <- tune(svm, y~., data=dat, kernel="linear", ranges=list(cost=c(0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1)))
summary(tune.out.1)

# test data set (try other than 10 nTestObs, compare to cost=0.01 below):
#nTestObs <- 10
nTestObs <- 1000
xtest <- matrix (rnorm (2*nTestObs*2) , ncol =2)
ytest <- sample (c(-1,1) , 2*nTestObs, rep=TRUE)
xtest[ytest ==1,] <- xtest[ytest ==1,] + 1
testdat <- data.frame(x=xtest, y=as.factor(ytest))

ypred <-  predict(bestmod,testdat)
# about 25% error rate:
table(predict=ypred, truth=testdat$y)

# cost=0.01
svmfit <- svm(y~., data=dat, kernel="linear", cost=0.01, scale=FALSE)
# 20 SVs:
plot(svmfit,dat)
ypred <- predict(svmfit, testdat)
# about 28% error rate:
table(predict=ypred, truth=testdat$y)
```

with n=2000 the effect of misclassification (slightly higher for `cost=0.01` than for `cost=0.1`) is more apparent  than two instead of one misclassification.


```{r Ch.9.6.1b}
# linearly (barely) separable case:
x[y==1,] <- x[y==1,]+0.5
plot(x, col=(y+5)/2, pch=(y+5)/2+15)

dat <- data.frame(x=x,y=as.factor(y))
# high cost for perfect separation:
svmfit <- svm(y~., data=dat, kernel ="linear", cost=1e5)
plot(svmfit,dat)
summary(svmfit)
# perfect:
table(predict(svmfit),y)

# test error on 20K observations:
nTestObs <- 10000
xtest <- matrix (rnorm (2*nTestObs*2) , ncol =2)
ytest <- sample (c(-1,1) , 2*nTestObs, rep=TRUE)
xtest[ytest ==1,] <- xtest[ytest ==1,] + 1 + 0.5
testdat <- data.frame(x=xtest, y=as.factor(ytest))

# about 15-18% error on test data:
ypred <-  predict(svmfit,testdat)
table(predict=ypred, truth=testdat$y)

# lower cost, more SVs, one misclassification:
svmfit <- svm(y~., data=dat, kernel="linear", cost=1)
plot(svmfit,dat)
summary(svmfit)
table(predict(svmfit),y)
# better performance on test data:
# about 13-15% error in test data:
ypred <-  predict(svmfit,testdat)
table(predict=ypred, truth=testdat$y)
```

In line with ISLR conclusion that model fit with `cost=1` *"will perform better on test data than the model with `cost=1e5`."*


# Ch.9.6.2 -- Support Vector Machine

```{r Ch.9.6.2}
set.seed(1)
nObs <- 100
x <- matrix(rnorm(2*nObs*2),ncol=2)
x[1:nObs,] <- x[1:nObs,]+2
x[nObs+(1:(nObs/2)),] <- x[nObs+(1:(nObs/2)),]-2
y <- c(rep(1,1.5*nObs),rep(2,nObs/2))
dat <- data.frame(x=x,y=as.factor(y))
# non-linear decision boundary:
plot(x,col=y,pch=y)

train <- sample(2*nObs,nObs)
# non-linear kernel, cost=1:
svmfit <- svm(y~., data=dat[train,], kernel="radial", gamma=1, cost=1)
plot(svmfit,dat[train,])
summary(svmfit)
table(predict(svmfit),dat[train,'y'])

# cost=100K, more irregular decision boundary, fewer SVs, lower training error:
svmfit <- svm(y~.,data=dat[train,],kernel="radial",gamma=1,cost=1e5)
plot(svmfit,dat[train,])
summary(svmfit)
table(predict(svmfit),dat[train,'y'])

# tune by cross-validation:
set.seed(1)
tune.out=tune(svm, y~., data=dat[train,], kernel="radial",ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))
# best cost=1, gamma=2:
summary (tune.out)
# 10/100 misclassifications:
table(true=dat[-train ,"y"],pred=predict(tune.out$best.model,newdata=dat[-train ,]))
# 13/100 misclassifications for high cost model:
table(true=dat[-train ,"y"],pred=predict(svmfit,newdata=dat[-train ,]))
```


# Ch.9.6.3 -- ROC curves

```{r Ch.9.6.3}
# wrap ROCR functions:
rocplot <- function(pred, truth, ...) {
  predob <- prediction(pred, truth)
  perf <- performance(predob, "tpr", "fpr")
  plot(perf ,...)
}

# best model:
svmfit.opt <- svm(y~., data=dat[train,], kernel="radial", gamma=2, cost=1)   ###, decision.values=TRUE)
plot(svmfit.opt,dat[train,])
table(predict(svmfit.opt),dat[train,"y"])

# more flexible model (higher gamma):
svmfit.flex <- svm(y~., data=dat[train,], kernel="radial", gamma=50, cost=1)   ###, decision.values=TRUE)
plot(svmfit.flex,dat[train,])
table(predict(svmfit.flex),dat[train,"y"])

# ROC curves for training and test predictions:
old.par <- par(mfrow=c(1,2))
fitted.opt <- attributes(predict(svmfit.opt ,dat[train,], decision.values =TRUE))$decision.values
fitted.flex <- attributes(predict(svmfit.flex,dat[train,], decision.values=TRUE))$decision.values
fitted.opt <- -1*sign(sum(fitted.opt*as.numeric(factor(dat[train,"y"]))))*fitted.opt
fitted.flex <- -1*sign(sum(fitted.flex*as.numeric(factor(dat[train,"y"]))))*fitted.flex
rocplot(fitted.opt,dat[train,"y"], main="Training Data")
rocplot(fitted.flex,dat[train,"y"], add=TRUE,col ="red ")
legend("bottomright",c("gamma=2","gamma=50"),col=1:2,text.col=1:2,lty=1) 
fitted.opt <- attributes(predict(svmfit.opt ,dat[-train,], decision.values =TRUE))$decision.values
fitted.flex <- attributes(predict(svmfit.flex,dat[-train,], decision.values=TRUE))$decision.values
fitted.opt <- -1*sign(sum(fitted.opt*as.numeric(factor(dat[-train,"y"]))))*fitted.opt
fitted.flex <- -1*sign(sum(fitted.flex*as.numeric(factor(dat[-train,"y"]))))*fitted.flex
rocplot(fitted.opt,dat[-train,"y"], main="Test Data")
rocplot(fitted.flex,dat[-train,"y"], add=TRUE,col ="red ")
legend("bottomright",c("gamma=2","gamma=50"),col=1:2,text.col=1:2,lty=1) 
par(old.par)
```

Notice how ROC curves swap their relative positions for training and test data for the models considered here

# Ch.9.6.4 -- SVM with multiple classes

```{r Ch.9.6.4}
set.seed(1)
# add points for the third class:
x <- rbind(x,matrix(rnorm(50*2),ncol=2))
y <- c(y,rep(0,50))
x[y==0,2] <- x[y==0,2]+2
dat <- data.frame(x=x,y=as.factor(y))
plot(x,col=(y+1),pch=(y+1))
svmfit <- svm(y~.,data=dat,kernel="radial",cost=10, gamma=1)
plot(svmfit,dat)
```

# Ch.9.6.5 -- gene expression data

```{r Ch.9.6.5}
names(Khan)
dim(Khan$xtrain)
dim(Khan$xtest)
length(Khan$ytrain)
length(Khan$ytest)
table(Khan$ytrain)
table(Khan$ytest)
dat <- data.frame(x=Khan$xtrain, y=as.factor(Khan$ytrain))
out <- svm(y~., data=dat, kernel="linear", cost=10)
summary(out)
table(out$fitted,dat$y)
dat.te <- data.frame(x=Khan$xtest, y=as.factor(Khan$ytest))
pred.te <- predict(out,newdata=dat.te)
table(pred.te,dat.te$y)

# pool training and test data together, obtained resampling-based test error estimate:
KhanAllDat <- data.frame(x=rbind(Khan$xtrain,Khan$xtest),y=as.factor(c(Khan$ytrain,Khan$ytest)))
# repeatedly draw training and test data stratified by cell type;
testPred <- NULL
testTruth <- NULL
nSim <- 100
for ( iSim in 1:nSim ) {
  trainIdx <- NULL
  varIdx <- c(sample(ncol(KhanAllDat)-1,ncol(KhanAllDat)/5),ncol(KhanAllDat))
  for ( iClass in levels(KhanAllDat$y)) {
    trainIdx <- c(trainIdx,sample((1:nrow(KhanAllDat))[KhanAllDat$y==iClass],sum(KhanAllDat$y==iClass),replace=TRUE))
  }
  svmTrain <- svm(y~., data=KhanAllDat[trainIdx,varIdx], kernel="linear", cost=10)
  #testPred <- c(testPred,attributes(predict(svmTrain,newdata=KhanAllDat[-trainIdx,], decision.values=TRUE))$decision.values)
  testPred <- c(testPred,predict(svmTrain,newdata=KhanAllDat[-trainIdx,varIdx]))
  testTruth <- c(testTruth,KhanAllDat[-trainIdx,"y"])
}
#rocplot(testPred,testTruth, main="Test Data")
table(pred=testPred,truth=testTruth)
table(pred=testPred,truth=testTruth)/nSim
plot(cmdscale(as.dist(1-cor(t(KhanAllDat[,-ncol(KhanAllDat)]),method="spearman"))),col=as.numeric(KhanAllDat$y))
```

On average about one observation gets misclassified

# Ch.9.7, Ex.4

*"Generate a simulated two-class data set with 100 observations and two features in which there is a visible but non-linear separation between the two classes. Show that in this setting, a support vector machine with a polynomial kernel (with degree greater than 1) or a radial kernel will outperform a support vector classifier on the training data. Which technique performs best on the test data? Make plots and report training and test error rates in order to back up your assertions."*

```{r Ch.9.7.Ex.4}
set.seed(123)
n <- 100
d <- 3
xyDat <- data.frame(x=c(rnorm(n/2),rnorm(n/4,mean=-d),rnorm(n/4,mean=d)),y=c(rnorm(n/2),rnorm(n/2,mean=d)),z=factor(c(rep(0,n/2),rep(1,n/2))))
plot(xyDat[,1:2],col=as.numeric(xyDat$z),pch=as.numeric(xyDat$z))
svmLin <- svm(z~., data=xyDat, kernel="linear")
table(xyDat$z,predict(svmLin))
plot(svmLin,xyDat)
svmRad <- svm(z~., data=xyDat, kernel="radial")
plot(svmRad,xyDat)
table(xyDat$z,predict(svmRad))
svmPoly <- svm(z~., data=xyDat, kernel="polynomial",coef0=1)
plot(svmPoly,xyDat)
table(xyDat$z,predict(svmPoly))
n <- 100000
dfTmp <- NULL
for ( iTry in 1:30 ) {
  xyTestDat <- data.frame(x=c(rnorm(n/2),rnorm(n/4,mean=-d),rnorm(n/4,mean=d)),y=c(rnorm(n/2),rnorm(n/2,mean=d)),z=factor(c(rep(0,n/2),rep(1,n/2))))
  tstTblLin <- table(xyTestDat$z,predict(svmLin,newdata = xyTestDat))
  tstTblRad <- table(xyTestDat$z,predict(svmRad,newdata = xyTestDat))
  tstTblPoly <- table(xyTestDat$z,predict(svmPoly,newdata = xyTestDat))
  #print(tstTblLin)
  #print(tstTblRad)
  #print(tstTblPoly)
  #cat("Test error:",1-sum(diag(tstTblLin))/n,1-sum(diag(tstTblRad))/n,1-sum(diag(tstTblPoly))/n,fill=TRUE)
  dfTmp <- rbind(dfTmp,data.frame(model=c("lin","rad","poly"),testerr=1-c(sum(diag(tstTblLin)),sum(diag(tstTblRad)),sum(diag(tstTblPoly)))/n))
}
ggplot(dfTmp,aes(x=model,y=log(testerr),colour=model))+geom_boxplot(fill="white",outlier.colour = NA)+geom_jitter()+theme_bw()
```

Linear kernel yields much higher error than polynomial or radial, and for the settings considered, polynomial might be marginally better than radial

# Ch.9.7, Ex.5

*"We have seen that we can fit an SVM with a non-linear kernel in order to perform classification using a non-linear decision boundary.We will now see that we can also obtain a non-linear decision boundary by performing logistic regression using non-linear transformations of the features."*

## Ch.9.7, Ex.5(a)

*"Generate a data set with n = 500 and p = 2, such that the observations belong to two classes with a quadratic decision boundary between them. For instance, you can do this as follows:"*

```{r Ch.9.7.Ex.5a}
x1=runif (500) -0.5
x2=runif (500) -0.5
y=1*( x1^2-x2^2 > 0)
```

## Ch.9.7, Ex.5(b)

*"Plot the observations, colored according to their class labels. Your plot should display X1 on the x-axis, and X2 on the yaxis."*

```{r Ch.9.7.Ex.5b}
plot(x1,x2,col=y+1,pch=y+1)
```

## Ch.9.7, Ex.5(c)

*"Fit a logistic regression model to the data, using X1 and X2 as predictors."*

```{r Ch.9.7.Ex.5c}
glmRes <- glm(Y~X1+X2,data.frame(Y=factor(y),X1=x1,X2=x2),family=binomial)
```

## Ch.9.7, Ex.5(d)

*"Apply this model to the __training data__ in order to obtain a predicted class label for each training observation. Plot the observations, colored according to the __predicted__ class labels. The decision boundary should be linear."*

```{r Ch.9.7.Ex.5d}
glmPred <- predict(glmRes,type="response")>0.5
table(y,glmPred)
old.par <- par(mfrow=c(1,2))
plot(x1,x2,col=y+1,pch=y+1)
plot(x1,x2,col=1+glmPred,pch=glmPred+1)
par(old.par)
```

## Ch.9.7, Ex.5(e)

*"Now fit a logistic regression model to the data using non-linear functions of $X_1$ and $X_2$ as predictors (e.g. $X_1^2$, $X_1 \times X_2$, $\log{X_2}$, and so forth)."*

```{r Ch.9.7.Ex.5e}
# I(X2^2)+
glmResNL <- glm(Y~X1+X2+I(X1^2)+X1:X2,data.frame(Y=factor(y),X1=x1,X2=x2),family=binomial)
```

## Ch.9.7, Ex.5(f)

*"Apply this model to the __training data__ in order to obtain a predicted class label for each training observation. Plot the observations, colored according to the __predicted class labels__. The decision boundary should be obviously non-linear. If it is not, then repeat (a)-(e) until you come up with an example in which the predicted class labels are obviously non-linear."*

```{r Ch.9.7.Ex.5f}
glmPredNL <- predict(glmResNL,type="response")>0.5
table(y,glmPredNL)
old.par <- par(mfrow=c(1,2))
plot(x1,x2,col=y+1,pch=y+1)
plot(x1,x2,col=1+glmPredNL,pch=glmPredNL+1)
par(old.par)
```

## Ch.9.7, Ex.5(g)

*"Fit a support vector classifier to the data with X1 and X2 as predictors. Obtain a class prediction for each training observation. Plot the observations, colored according to the __predicted class labels__."*

```{r Ch.9.7.Ex.5g}
svmLin <- svm(Y~., data=data.frame(Y=factor(y),X1=x1,X2=x2),cost=1e3, kernel="linear")
plot(svmLin,data.frame(Y=factor(y),X1=x1,X2=x2))
table(y,predict(svmLin))
```

## Ch.9.7, Ex.5(h)

*"Fit a SVM using a non-linear kernel to the data. Obtain a class prediction for each training observation. Plot the observations, colored according to the __predicted class labels__."*


```{r Ch.9.7.Ex.5h}
svmRad <- svm(Y~., data=data.frame(Y=factor(y),X1=x1,X2=x2), kernel="radial")
plot(svmRad,data.frame(Y=factor(y),X1=x1,X2=x2))
table(y,predict(svmRad))
```


# Ch.9.7, Ex.7

*"In this problem, you will use support vector approaches in order to predict whether a given car gets high or low gas mileage based on the `Auto` data set."*

```{r}
class(Auto)
dim(Auto)
summary(Auto)
```

## Ch.9.7, Ex.7 a)

*"Create a binary variable that takes on a 1 for cars with gas mileage above the median, and a 0 for cars with gas mileage below the median."*

```{r}
Auto$mpgCtg <- factor(Auto$mpg>median(Auto$mpg))
```

## Ch.9.7, Ex.7 b)

*"Fit a support vector classifier to the data with various values of `cost`, in order to predict whether a car gets high or low gas mileage. Report the cross-validation errors associated with different values of this parameter. Comment on your results."*

```{r}
for ( iTry in 1:3 ) {
  tune.out=tune(svm, mpgCtg~., data=Auto[,c("cylinders","displacement","horsepower","weight","acceleration","year","mpgCtg")], kernel="linear",ranges=list(cost=c(0.1,1,10,100,1000)))
  print(tune.out$best.parameters)
  print(tune.out$best.performance)
}
print(summary(tune.out))
```

Lowest error for range of cost values tested lowest error appears to be about the same considering its dispersion -- the actual minimum varies across different trials of cross-validation

## Ch.9.7, Ex.7 c)

*"Now repeat (b), this time using SVMs with radial and polynomial basis kernels, with different values of `gamma` and `degree` and `cost`. Comment on your results."*

### radial

```{r}
for ( iTry in 1:3 ) {
  tune.out=tune(svm, mpgCtg~., data=Auto[,c("cylinders","displacement","horsepower","weight","acceleration","year","mpgCtg")], kernel="radial",ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))
  print(tune.out$best.parameters)
  print(tune.out$best.performance)
}
summary(tune.out)
for ( iTry in 1:3 ) {
  tune.out=tune(svm, mpgCtg~., data=Auto[,c("cylinders","displacement","horsepower","weight","acceleration","year","mpgCtg")], kernel="radial",ranges=list(cost=c(0.1,0.2,0.5,1,2,5,10),gamma=c(1,1.5,2,2.5,3)))
  print(tune.out$best.parameters)
  print(tune.out$best.performance)
}
summary(tune.out)
```

Roughly best performance by cross-validation seems to be achievable for low single-digits values of `cost` and `gamma`.  Average performance for radial kernel as estimated by cross-validation appears to be better than that for linear kernel / SVC

### polynomial

```{r}
for ( iTry in 1:3 ) {
  tune.out=tune(svm, mpgCtg~., data=Auto[,c("cylinders","displacement","horsepower","weight","acceleration","year","mpgCtg")], kernel="polynomial",ranges=list(cost=c(0.1,1,10,100,1000),degree=c(2,3),coef0=c(0,0.5,1,2,3,4)))
  print(tune.out$best.parameters)
  print(tune.out$best.performance)
}
summary(tune.out)
for ( iTry in 1:10 ) {
  tune.out=tune(svm, mpgCtg~., data=Auto[,c("cylinders","displacement","horsepower","weight","acceleration","year","mpgCtg")], kernel="polynomial",ranges=list(cost=c(5,10,20,50,100,200),degree=c(2,3),coef0=c(0,0.5,1,1.5,2)))
  print(tune.out$best.parameters)
  print(tune.out$best.performance)
}
summary(tune.out)
```

There is more variability in best parameters choice for the polynomial kernel; the best model performance could be a bit worse than that for radial kernel

## Ch.9.7, Ex.7 d)

*"Make some plots to back up your assertions in (b) and (c)."*

```{r}
svmfit <- svm(mpgCtg~., data=Auto[,c("cylinders","displacement","horsepower","weight","acceleration","year","mpgCtg")], kernel="linear", cost=10)
plot(svmfit, Auto, year~weight)
plot(svmfit, Auto, acceleration~cylinders)
plot(svmfit, Auto, horsepower~displacement)
svmfit <- svm(mpgCtg~., data=Auto[,c("cylinders","displacement","horsepower","weight","acceleration","year","mpgCtg")], kernel="radial", cost=2,gamma=2)
plot(svmfit, Auto, year~weight)
plot(svmfit, Auto, acceleration~cylinders)
plot(svmfit, Auto, horsepower~displacement)
svmfit <- svm(mpgCtg~., data=Auto[,c("weight","year","mpgCtg")], kernel="radial", cost=2,gamma=2)
plot(svmfit, Auto, year~weight)
svmfit <- svm(mpgCtg~., data=Auto[,c("weight","year","mpgCtg")], kernel="radial", cost=20,gamma=20)
plot(svmfit, Auto, year~weight)
svmfit <- svm(mpgCtg~., data=Auto[,c("cylinders","displacement","horsepower","weight","acceleration","year","mpgCtg")], kernel="radial", cost=20,gamma=20)
plot(svmfit, Auto, year~weight)
summary(glm(mpgCtg~.,data=Auto[,c("cylinders","displacement","horsepower","weight","acceleration","year","mpgCtg")],family=binomial))
```

The time it took to knit this file from beginning to end is about (seconds):

```{r}
proc.time() - ptStart
```
