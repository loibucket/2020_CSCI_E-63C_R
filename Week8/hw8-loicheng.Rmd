---
title: 'CSCI E-63C: Week 8 Problem Set | Loi Cheng'
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
library(cluster)
library(ISLR)
library(MASS)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
options(width = 200)
```

# Preface

In this problem set we will exercise some of the unsupervised learning approaches on [2018 Global Health Observatory (GHO) data](https://www.who.int/gho/publications/world_health_statistics/2018/en/).  It is available at that website in the form of [Excel file](https://www.who.int/gho/publications/world_health_statistics/2018/whs2018_AnnexB.xls?ua=1), but its cleaned up version ready for import into R for further analyses is available at CSCI E-63C canvas course web site [whs2018_AnnexB-subset-wo-NAs.txt](https://canvas.harvard.edu/files/9283256/download?download_frd=1).  The cleaning and reformatting included: merging data from the three parts of Annex B, reducing column headers to one line with short tags, removal of ">", "<" and whitespaces, conversion to numeric format, removal of the attributes with more than 20% of missing values and imputing the remaining missing values to their respective medians.  You are advised to save yourself that trouble and start from preformatted text file available at the course website as shown above.  The explicit mapping of variable names to their full description as provided in the original file is available in Excel file [whs2018_AnnexB-subset-wo-NAs-columns.xls](https://canvas.harvard.edu/files/9283257/download?download_frd=1) also available on the course canvas page.  Lastly, you are advised to download a local copy of this text file to your computer and access it there (as opposed to relying on R ability to establish URL connection to canvas that potentially requires login etc.)

Short example of code shown below illustrates reading this data from a local copy on your computer (assuming it has been copied into current working directory of your R session -- `getwd()` and `setwd()` commands are helpful to find out what is it currently and change it to desired location) and displaying summaries and pairs plot of five (out of almost 40) arbitrary chosen variables.  This is done for illustration purposes only -- the problems in this set expect use of all variables in this dataset.

```{r WHS,fig.height=10,fig.width=10}
whs2018annexBdat <- read.table("whs2018_AnnexB-subset-wo-NAs.txt",sep="\t",header=TRUE,quote="")
summary(whs2018annexBdat[,c(1,4,10,17,26)])
pairs(whs2018annexBdat[,c(1,4,10,17,26)])
```

In a way this dataset is somewhat similar to the `USArrests` dataset extensively used in ISLR labs and exercises -- it collects various continuous statistics characterizing human population across different territories.  It is several folds larger though -- instead of `r nrow(USArrests)` US states and `r ncol(USArrests)` attributes in `USArrests`, world health statistics (WHS) data characterizes `r nrow(whs2018annexBdat)` WHO member states by `r ncol(whs2018annexBdat)` variables.  Have fun!

The following problems are largely modeled after labs and exercises from Chapter 10 ISLR.  If anything presents a challenge, besides asking questions on piazza (that is always a good idea!), you are also encouraged to review corresponding lab sections in ISLR Chapter 10.

# Problem 1: Principal components analysis (PCA) (25 points)

The goal here is to appreciate the impact of scaling of the input variables on the result of the principal components analysis.  To that end, you will first survey means and variances of the attributes in this dataset (sub-problem 1a) and then obtain and explore results of PCA performed on data as is and after centering and scaling each attribute to zero mean and standard deviation of one (sub-problem 1b).

## Sub-problem 1a: means and variances of WHS attributes (5 points)

Compare means and variances of the *untransformed* attributes in the world health statisics dataset -- plot of variance vs. mean is probably the best given the number of attributes in the dataset.  Function `apply` allows to apply desired function (e.g. `mean` or `var` or `sd`) to each row or column in the table.  Do you see all `r ncol(whs2018annexBdat)` attributes in the plot, or at least most of them?  (Remember that you can use `plot(inpX,inpY,log="xy")` to use log-scale on both horizontal and vertical axes.)  Is there a dependency between attributes' averages and variances? What is the range of means and variances when calculated on untransformed data?  Which are the top two attributes with the highest mean or variance?  What are the implications for PCA rendition of this dataset (in two dimensions) if applied to untransformed data?

```{r}
head(whs2018annexBdat)
```
```{r}
means = apply(whs2018annexBdat,2,mean)
variances = apply(whs2018annexBdat,2,var)
```

```{r, fig.width=10,fig.height=5}
op = par(mfrow=c(1,2))
#non-log scale
plot(means,variances,xlim=c(1,300),ylim=c(1,10^5),main="untransformed axis")
#log scale
plot(means,variances,log="xy",xlim=c(1,10^15),ylim=c(1,10^15),main="log axis")
par(op)
```
Is there a dependency between attributes’ averages and variances? 
**The averages and variances have a positive correlation.**

What is the range of means and variances when calculated on untransformed data?
```{r}
min(means)
max(means)
min(variances)
max(variances)
```
**For means, the range is 0.1948454 to 7732495.**
**For variances, the range is 0.1313747 to 1.28799e+15.**

Which are the top two attributes with the highest mean or variance? 

```{r}
sort(means, decreasing = TRUE)
```

```{r}
sort(variances, decreasing = TRUE)
```
**NTDinterventions and TotalPopulation have the highest means and variances.**

What are the implications for PCA rendition of this dataset (in two dimensions) if applied to untransformed data?
**With untransformed data, the principal component that maximizes NTDinterventions variances would have a much larger scale compared to the others.**

## Sub-problem 1b: PCA on untransformed and scaled WHS data (20 points)

Perform the steps outlined below *both* using *untransformed* data and *scaled* attributes in WHS dataset (remember, you can use R function `prcomp` to run PCA and to scale data you can either use as input to `prcomp` the output of `scale` as applied to the WHS data matrix or call `prcomp` with parameter `scale` set to `TRUE`). To make it explicit, the comparisons outlined below have to be performed first on the unstransformed WHS data and then again on scaled WHS data -- you should obtain two sets of results that you could compare and contrast.

1. Obtain results of principal components analysis of the data (by using `prcomp`)
```{r}
#df = data.frame(means=means,variances=variances)
pu = prcomp(whs2018annexBdat,retx=TRUE)
pu
```

```{r}
ps = prcomp(whs2018annexBdat,retx=TRUE,scale=TRUE)
ps
```

2. Generate scree plot of PCA results (by calling `plot` on the result of `prcomp`)
```{r, fig.width=10,fig.height=5}
op = par(mfrow=c(1,2))
plot(pu)
plot(ps)
par(op)
```
**With untransformed data, one component has much larger variances compared with the others, which is probably the PC for NTDinterventions.  This difference is reduced with the scaled data.**

```{r, fig.width=10,fig.height=5}
op = par(mfrow=c(1,2))
biplot(pu)
biplot(ps)
par(op)
```

3. Generate plot of the two first principal components using `biplot`.  Which variables seem to predominantly drive the results of PCA when applied to untransformed data?
**When applied to the untransformed data, NTDinterventions appear to predominantly drive the PCA results **

  + Please note that in case of untransformed data you should expect `biplot` to generate substantial number of warnings.  Usually in R we should pay attention to these and understand whether they indicate that something went wrong in our analyses.  In this particular case they are expected -- why do you think that is?
**The error "zero-length arrow is of indeterminate angle and so skipped" appears to be a results of the untransformed plot.  As shown in the scree plot, only the first component has significant variance, and all the others have essentially zero variance, which results in zero length arrows.**  
  
4. The field `rotation` in the output of `prcomp` contains *loadings* of the 1st, 2nd, etc. principal components (PCs) -- that can interpreted as contributions of each of the attributes in the input data to each of the PCs.
  + What attributes have the largest (by their absolute value) loadings for the first and second principal component?
```{r}
#PC1, untransformed
n = names(sort(abs(pu$rotation[,1]),decreasing=TRUE)[c(1:4)])
pu$rotation[,1][n]
```
**PC1 is dominated by NTDinterventions.  Increasing NTDinterventions should also increase PC1.**

```{r}
#PC1, scaled
n = names(sort(abs(ps$rotation[,1]),decreasing=TRUE)[c(1:4)])
ps$rotation[,1][n]
```
**PC1 seems related to life expectancy.  Decreasing life expectancy increases PC1, while increasing child mortality increases PC1.  It seems PC1 relates to death rate, which increases with childmortality, and decreases with life expectancy.**

**For PC1, comparing untransformed and scaled, they have entirely different top loadings.**

```{r}
#PC2, untransformed
n = names(sort(abs(pu$rotation[,2]),decreasing=TRUE)[c(1:4)])
pu$rotation[,2][n]
```
**PC2 is dominated by TotalPopulation.    Increasing TotalPopulation should also increase PC2.**

```{r}
#PC2, scaled
n = names(sort(abs(ps$rotation[,2]),decreasing=TRUE)[c(1:4)])
ps$rotation[,2][n]
``` 
**Decreasing health expedentures, alcohol consumption, and nurses appears to increase PC2.  Perhaps low alcohol consumption means healthier people, and so less health expeditures, and fewer nurses are requried.  So, these four variables appear to be highly correlated.**

**For PC2, both have CHEperCapita as a top loading, but the other 3 are different.**

  + How does it compare to what you have observed when comparing means and variances of all attributes in the world health statistics dataset?
**If untransformed, the variables with top loadings correspond to the highest variances.  When scaled, there is no apparent relationship between loadings and variances** 

5. Calculate percentage of variance explained (PVE) by the first five principal components (PCs).  You can find an example of doing this in ISLR Chapter 10.4 (Lab 1 on PCA).

```{r}
puvar = pu$sdev^2
puvar[1:5]/sum(puvar)
```
**When unscaled, PC1 explains almost all of the variances at 99%.  All the other components are nearly zero.**
```{r}
psvar = ps$sdev^2
psvar[1:5]/sum(psvar)
```
**When unscaled, PC1 explains 47% of the variances.  Other components have a lower value, e.g. PC2 = 7%, but are also significant **

Now that you have PCA results when applied to untransformed and scaled WHS data, please comment on how do they compare and what is the effect of scaling?  

**Scaling resolved the problem of having one variable with much larger variance dominate the loadings of components.  The PCA results of the unscaled data is unusuable other than to study NTDinterventions.**

What dataset attributes contribute the most (by absolute value) to the top two principal components in each case (untransformed and scaled data)?  What are the signs of those contributions?  **See Part 4**

How do you interpret that? **See Part 4**

Please note, that the output of `biplot` with almost 200 text labels on it can be pretty busy and tough to read.  You can achieve better control when plotting PCA results if instead you plot the first two columns of the `x` attribute in the output of `prcomp` -- e.g. `plot(prcomp(USArrests,scale=T)$x[,1:2])`.  Then given this plot you can label a subset of countries on the plot by using `text` function in R to add labels at specified positions on the plot.  Please feel free to choose several countries of your preference and discuss the results.  Alternatively, indicate US, UK, China, India, Mexico, Australia, Israel, Italy, Ireland and Sweden and discuss the results.  Where do the countries you have plotted fall in the graph?  Considering what you found out about contributions of different attributes to the first two PCs, what do their positions tell us about their (dis-)similarities in terms of associated health statistics?

```{r, fig.width=10,fig.height=10}
plot(ps$x[,1:2])
coun = c('United States of America', 'United Kingdom', 'China', 'India', 'Mexico', 'Australia', 'Israel', 'Italy', 'Ireland', 'Sweden')
labels = ps$x[coun,1:2]
text(labels[,1],labels[,2],row.names(labels))
```
**The plot suggests that the countries that are close together have similar health statistics.  So, US, Sweden, Ireland, Australia and UK have similar health statistics.  These countries have very different statistics when compared with Mexico or China, which are much farther away in the plot.**

# Problem 2: K-means clustering (20 points)

The goal of this problem is to practice use of K-means clustering and in the process appreciate the variability of the results due to different random starting assignments of observations to clusters and the effect of parameter `nstart` in alleviating it.

## Sub-problem 2a: k-means clusters of different size (5 points)

Using function `kmeans` perform K-means clustering on *explicitly scaled* (e.g. `kmeans(scale(x),2)`) WHS data for 2, 3 and 4 clusters.  Use `cluster` attribute in the output of `kmeans` to indicate cluster membership by color and/or shape of the corresponding symbols in the plot of the first two principal components generated independently on the same (scaled WHS) data.  E.g. `plot(prcomp(xyz)$x[,1:2],col=kmeans(xyz,4)$cluster)` where `xyz` is input data.  Describe the results.  Which countries are clustered together for each of these choices of $K$?

```{r}
#kmeans(scale(whs2018annexBdat),2)
```

```{r}
p = prcomp(whs2018annexBdat,scale=TRUE,retx=TRUE)
labels = p$x[seq(1, nrow(p$x), 6),1:2]
```

```{r fig.width=15,fig.height=15}
plot(p$x[,1:2],col=kmeans(scale(whs2018annexBdat),2)$cluster)
text(labels[,1],labels[,2],row.names(labels))
```

**For 2 clusters, some Middle East and Africa countries are grouped together in one cluster, like Afganistan and Ghana.  These countries have high PC1, which may indicate lower quality health statistics.  Asian, European and South American countries are grouped together in another cluster, like Finland, Denmark, Korea.  These countries have low PC1, which probably indicates high quality health statistics.**

```{r fig.width=15,fig.height=15}
plot(p$x[,1:2],col=kmeans(scale(whs2018annexBdat),3)$cluster)
text(labels[,1],labels[,2],row.names(labels))
```

**For 3 clusters, again some Middle East and Africa countries are grouped together in one cluster, like Afghanistan and Ghana.  These countries have high PC1, and probably have the lowest quality health statistics.  Some South American and South Asian countries are grouped in another cluster, like Argentina and Malaysia.  This group have higher PC2 compared with other clusters. High PC2 correlates with low health expenditures and low alcohol consumption.  European and some Asian countries like Denmark and Korea are in another cluster, which seems to be more first-world, like having high standards of living.**

```{r fig.width=15,fig.height=15}
labels = p$x[seq(1, nrow(p$x), 4),1:2]
plot(p$x[,1:2],col=kmeans(scale(whs2018annexBdat),4)$cluster)
text(labels[,1],labels[,2],row.names(labels))
```

**For 4 clusters, again some European and some Asian countries like Denmark and Korea are in a cluster, which seems to be more first-world, like having high standards of living.  These countries are also in the Northern Hemisphere.  The secound cluster has South American and Eastern European countries, like Romania and Paraguay.  These countries are mostly about the equator or in the Southern Hemisphere.  They seem to have lower quality health statistics than the first cluster.  The third cluster has Middle Eastern countries like Iraq, and some South Asian countries like Laos.  These countries likely have lower quality health statistics than the second group.  The last cluster has Middle East countries like Afghanistan and African countries like Nigeria.  These countries probably have the lowest quality health statistics out of all the clusters.**

## Sub-problem 2b: variability of k-means clustering and effect of `nstart` parameter (15 points)

By default, k-means clustering uses random set of centers as initial guesses of cluster centers.  Here we will explore variability of k-means cluster membership across several such initial random guesses.  To make such choices of random centers reproducible, we will use function `set.seed` to reset random number generator (RNG) used in R to make those initial guesses to known/controlled initial state.

Using the approach defined above, repeat k-means clustering of *explicitly scaled* WHS data with four (`centers=4`) clusters three times resetting RNG each time with `set.seed` using seeds of 1, 2 and 3 respectively (and default value of `nstart=1`).  Indicate cluster membership in each of these three trials on the plot of the first two principal components using color and/or shape as described above.  Two fields in the output of `kmeans` -- `tot.withinss` and `betweenss` -- characterize within and between clusters sum-of-squares.  Tighter clustering results are those which have smaller ratio of within to between sum-of-squares.  What are the resulting ratios of within to between sum-of-squares for each of these three k-means clustering results (with random seeds of 1, 2 and 3)?

```{r fig.width=15,fig.height=15}

op=par(mfrow=c(3,3))

#nstart
n=1

#seed1
s=1
set.seed(s)
labels = p$x[seq(1, nrow(p$x), 21),1:2]
k=kmeans(scale(whs2018annexBdat),4,nstart=n)
w = k$tot.withinss
b = k$betweenss
plot(p$x[,1:2],col=k$cluster,main=paste("nstart=",n,"seed=",s,"w/b=",w/b))
text(labels[,1],labels[,2],row.names(labels))


#seed2
s=2
set.seed(s)
labels = p$x[seq(1, nrow(p$x), 21),1:2]
k=kmeans(scale(whs2018annexBdat),4,nstart=n)
w = k$tot.withinss
b = k$betweenss
plot(p$x[,1:2],col=k$cluster,main=paste("nstart=",n,"seed=",s,"w/b=",w/b))
text(labels[,1],labels[,2],row.names(labels))

#seed3
s=3
set.seed(s)
labels = p$x[seq(1, nrow(p$x), 21),1:2]
k=kmeans(scale(whs2018annexBdat),4,nstart=n)
w = k$tot.withinss
b = k$betweenss
plot(p$x[,1:2],col=k$cluster,main=paste("nstart=",n,"seed=",s,"w/b=",w/b))
text(labels[,1],labels[,2],row.names(labels))

##################################
#nstart
n=11

#seed1
s=1
set.seed(s)
labels = p$x[seq(1, nrow(p$x), 21),1:2]
k=kmeans(scale(whs2018annexBdat),4,nstart=n)
w = k$tot.withinss
b = k$betweenss
plot(p$x[,1:2],col=k$cluster,main=paste("nstart=",n,"seed=",s,"w/b=",w/b))
text(labels[,1],labels[,2],row.names(labels))


#seed2
s=2
set.seed(s)
labels = p$x[seq(1, nrow(p$x), 21),1:2]
k=kmeans(scale(whs2018annexBdat),4,nstart=n)
w = k$tot.withinss
b = k$betweenss
plot(p$x[,1:2],col=k$cluster,main=paste("nstart=",n,"seed=",s,"w/b=",w/b))
text(labels[,1],labels[,2],row.names(labels))

#seed3
s=3
set.seed(s)
labels = p$x[seq(1, nrow(p$x), 21),1:2]
k=kmeans(scale(whs2018annexBdat),4,nstart=n)
w = k$tot.withinss
b = k$betweenss
plot(p$x[,1:2],col=k$cluster,main=paste("nstart=",n,"seed=",s,"w/b=",w/b))
text(labels[,1],labels[,2],row.names(labels))

##################################
#nstart
n=100

#seed1
s=1
set.seed(s)
labels = p$x[seq(1, nrow(p$x), 21),1:2]
k=kmeans(scale(whs2018annexBdat),4,nstart=n)
w = k$tot.withinss
b = k$betweenss
plot(p$x[,1:2],col=k$cluster,main=paste("nstart=",n,"seed=",s,"w/b=",w/b))
text(labels[,1],labels[,2],row.names(labels))


#seed2
s=2
set.seed(s)
labels = p$x[seq(1, nrow(p$x), 21),1:2]
k=kmeans(scale(whs2018annexBdat),4,nstart=n)
w = k$tot.withinss
b = k$betweenss
plot(p$x[,1:2],col=k$cluster,main=paste("nstart=",n,"seed=",s,"w/b=",w/b))
text(labels[,1],labels[,2],row.names(labels))

#seed3
s=3
set.seed(s)
labels = p$x[seq(1, nrow(p$x), 21),1:2]
k=kmeans(scale(whs2018annexBdat),4,nstart=n)
w = k$tot.withinss
b = k$betweenss
plot(p$x[,1:2],col=k$cluster,main=paste("nstart=",n,"seed=",s,"w/b=",w/b))
text(labels[,1],labels[,2],row.names(labels))

par(op)

```
w/b ratio: ratio of within to between sum-of-squares
**For nstart=1, with seed=1 or seed=3, the w/b ratio is about 1.04.  With seed=2, the w/b ratio is about 1.1.**

**For seed=1, the clusters are distinctly separated from one another.  However, for seed=2, the black and red clusters apears to be mixed together.  For seed=3, one of the clusters, the blue one, is almost non-existent, with only 2 points.**

Please bear in mind that the actual cluster identity is assigned randomly and does not matter -- i.e. if cluster 1 from the first run of `kmeans` (with random seed of 1) and cluster 4 from the run with the random seed of 2 contain the same observations (country/states in case of WHS dataset), they are *the same* clusters.

Repeat the same procedure (k-means with four clusters for RNG seeds of 1, 2 and 3) now using `nstart=100` as a parameter in the call to `kmeans`.  Represent results graphically as before.  How does cluster membership compare between those three runs now?  What is the ratio of within to between sum-of-squares in each of these three cases?  What is the impact of using higher than 1 (default) value of `nstart`?  What is the ISLR recommendation on this offered in Ch. 10.5.1?

**With increases to nstart, the clusters become more stable.  For nstart=1, the plots with different seeds show different clustering arrangements.  There are also differences in the w/b ratio in range of about about 1.04 to 1.1.  The plots of nstart=100 shows nearly identical clusters for all 3 seeds, and w/b for each is identical at about 1.04.**

One way to achieve everything this sub-problem calls for is to loop over `nstart` values of 1 and 100, for each value of `nstart`, loop over RNG seeds of 1, 2 and 3, for each value of RNG seed, reset RNG, call `kmeans` and plot results for each combination of `nstart` and RNG seed value.


# Problem 3: Hierarchical clustering (15 points)

## Sub-problem 3a: hierachical clustering by different linkages (10 points)

Cluster country states in (scaled) world health statistics data using default (Euclidean) distance and "complete", "average", "single" and "ward" linkages in the call to `hclust`.  Plot each clustering hierarchy, describe the differences.  For comparison, plot results of clustering *untransformed* WHS data using default parameters (Euclidean distance, "complete" linkage) -- discuss the impact of the scaling on the outcome of hierarchical clustering.

```{r fig.width=19*1,fig.height=19*5}
op = par(mfrow=c(5,1))

h = hclust(dist(scale(whs2018annexBdat)),method="complete")
plot(h,main="Cluster Dendrogram, Scaled, Method=complete")

h = hclust(dist(scale(whs2018annexBdat)),method="average")
plot(h,main="Cluster Dendrogram, Scaled, Method=average")

h = hclust(dist(scale(whs2018annexBdat)),method="single")
plot(h,main="Cluster Dendrogram, Scaled, Method=single")

h = hclust(dist(scale(whs2018annexBdat)),method="ward.D")
plot(h,main="Cluster Dendrogram, Scaled, Method=ward")

h = hclust(dist(whs2018annexBdat),method="complete")
plot(h,main="Cluster Dendrogram, Unscaled")

par(op)
```


```{r}
ord = order(whs2018annexBdat["NTDinterventions"],decreasing=TRUE)[1:5]
whs2018annexBdat[ord,]
```

**For the complete method, the trees are somewhat unbalanced, with some countries as independent nodes at higher levels, such as India, Nepal, Syrian Arab Republic, and China.  It appears that India and Ukraine are the farthest apart.  At level between 10 and 15, there are 4 branches that are attached to major clusters.  The leftmost cluster has many African countries, such as Sudan and Uganda.  It also has some Middle East countries like Afghanistan.  The second from left cluster has mostly European countries like Spain and Italy.  It also has the US and Japan, which all combined seems like a cluster of “first world” countries.  The third from left is the largest cluster and has a mix of South American, Eastern Europe, and South Asian countries, like Chile, Romania, and Vietnam.  The last cluster seems to have many islands, like Philippines, Cook Islands, and Indonesia.**

**For the average tree appears skewed with many nodes on the right side and few on the left.  Similar to the complete method, many African countries are close together, followed by a group of “first world” countries.  The next set has many South American countries, but unlike the complete method, there are fewer South Asian countries, which appears more towards the right end along with many islands.**

**Compared to the average tree, the single tree is even more skewed towards the right.  Many of the left branches only have one node.  There are some set of countries close to each other like the previous methods.  For example, European countries like Norway, Denmark, and France and close to each other.  Many African countries are also close together.  Unlike the other previous trees, the rest of the countries, e.g. North/South America, Middle East, and Asia are all intermixed together on the left half of the diagram.**

**The ward tree appears to be the most balanced of all the trees, with fairly similar number of nodes on the right and left side.  The arrangement of the countries are also very different from the others.  Many European countries like Spain and Italy, and other “first world” countries like the US and Japan are near each other on the right end of the diagram.  On the left end, there are many African countries.  In the middle is a mix of the rest of the countries that do not form any groups like those identified in the other trees.**

**With unscaled data, India is in its own cluster.  This is probably due to the NTDinterventions value, which has by far the largest max value across all the variables.  For India, the NTDinterventions value is about 4x the second country, Nigeria.  So, the distances are mainly attributed from NTDinterventions, with all other variables having much lower loadings.**

## Sub-problem 3b: compare k-means and hierarchical clustering (5 points)

Using function `cutree` on the output of `hclust` determine assignment of the countries in WHS dataset into top four clusters when using Euclidean distance and Ward linkage. (Feel free to choose which one of the two varieties of Ward linkage available in `hclust` you want to use here!).  Use function `table` to compare membership of these clusters to those produced by k-means clustering with four clusters in the Problem 2(b) when using `nstart=100` (and any of the RNG seeds) above.  Discuss the results.

```{r fig.width=28*1,fig.height=28*1}

op=par(mfrow=c(1,1))

h = hclust(dist(scale(whs2018annexBdat)),method="ward.D")
c = cutree(h,k=4)

plot(h,main="Cluster Dendrogram, Scaled, Method=ward")
rect.hclust(h,k=4)

par(op)

```


```{r fig.width=10*2,fig.height=10*1}

set.seed(4)

op=par(mfrow=c(1,2))
k=kmeans(scale(whs2018annexBdat),4,nstart=100)

labels = p$x[seq(1, nrow(p$x), 21),1:2]
plot(p$x[,1:2],col=c,main="hierachical clustering")
text(labels[,1],labels[,2],row.names(labels))

plot(p$x[,1:2],col=k$cluster,main="k-means")
text(labels[,1],labels[,2],row.names(labels))
par(op)

```

**Hierachical clustering and k-means produced mostly similar clustering results, with some differences.  For example, the right most cluster (black), the k-means cluster is slightly larger than the hierachial cluster.  The k-means blue cluster at the top has just 2 points, while the hierachial extends from the top to the middle of the plot.**

```{r}
table(c)
v = sort(c)

table(k$cluster)
kv = sort(k$cluster)
```

**Clusters 1 and 3 have comparable number of points, while the kmeans cluster 2 has significantly more points (20), and the kmeans cluster 4 has significantly fewer points (25).  The table below shows which countries share the same cluster (TRUE), and which ones are different (FALSE)**

```{r}
df = data.frame(c, kv)
df$same = df$c == df$kv
df
```



